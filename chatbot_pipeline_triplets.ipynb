{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "RfXfYb8XVg6Y",
   "metadata": {
    "id": "RfXfYb8XVg6Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: pinecone-client in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from sentence_transformers) (4.34.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from sentence_transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from sentence_transformers) (0.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from sentence_transformers) (1.22.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from sentence_transformers) (1.3.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from sentence_transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from sentence_transformers) (0.16.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from pinecone-client) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from pinecone-client) (6.0.1)\n",
      "Requirement already satisfied: loguru>=0.5.0 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from pinecone-client) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from pinecone-client) (4.7.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from pinecone-client) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from pinecone-client) (1.26.16)\n",
      "Requirement already satisfied: filelock in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from loguru>=0.5.0->pinecone-client) (0.4.6)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from loguru>=0.5.0->pinecone-client) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from requests>=2.19.0->pinecone-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
      "Requirement already satisfied: sympy in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from nltk->sentence_transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from nltk->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from torchvision->sentence_transformers) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: neo4j in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (5.14.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages (from neo4j) (2022.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers pinecone-client\n",
    "!pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52f7baa4",
   "metadata": {
    "id": "52f7baa4"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import os, json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "886433d1",
   "metadata": {
    "id": "886433d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\.conda\\envs\\langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from tqdm import tqdm\n",
    "import requests, re\n",
    "import pinecone\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "GGfnex56EZqm",
   "metadata": {
    "id": "GGfnex56EZqm"
   },
   "outputs": [],
   "source": [
    "import neo4j\n",
    "from neo4j import GraphDatabase\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "NTx9IAy7EmUC",
   "metadata": {
    "id": "NTx9IAy7EmUC"
   },
   "outputs": [],
   "source": [
    "#triplet retrieval pipeline parameters\n",
    "\n",
    "url = \"neo4j+s://ef25c60e.databases.neo4j.io:7687\"\n",
    "username =\"username\"\n",
    "password = \"password\"\n",
    "\n",
    "graphDB_Driver = GraphDatabase.driver(url, auth=(username, password))\n",
    "\n",
    "# rel_str = 'ActTowards, CommunicatesWith, InteractsWith, Supplies, Demands, Acquires, Transforms, Decides, Assesses, Solves, Develops, Impacts, Manages, Moves, Happens'\n",
    "# node_str = 'Person, Company, Organization, Facility, Location, GeoPoliticalEntity, Time, Date, Event, Product, Regulation'\n",
    "\n",
    "rel_str = 'ActTowards, CommunicatesWith, InteractsWith, Supplies, Demands, Acquires, Transforms, Decides, Assesses, Solves, Develops, Impacts, Manages, Moves, Happens, Produce, is'\n",
    "node_str = 'Person, Company, Organization, Facility, Location, Country, GeoPoliticalEntity, Time, Date, Event, Product, Regulation, Journal, Paper, Number, Problem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Rg21c8SiEyLR",
   "metadata": {
    "id": "Rg21c8SiEyLR"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#triplet retrieval\n",
    "def find_match_query(input_string):\n",
    "    pattern1 = re.compile(r'MATCH.*?\\n', re.DOTALL)\n",
    "    match1 = pattern1.search(input_string)\n",
    "    pattern2 = re.compile(r'MATCH.*?\\.', re.DOTALL)\n",
    "    match2 = pattern2.search(input_string)\n",
    "\n",
    "    if match1 and not match2:\n",
    "        return match1.group().rstrip().rstrip('.').rstrip(',')\n",
    "    elif match2 and not match1:\n",
    "        return match2.group().rstrip().rstrip('.').rstrip(',')\n",
    "    elif not match1 and not match2:\n",
    "        return None\n",
    "    elif len(match1.group()) < len(match2.group()):\n",
    "        return match1.group().rstrip().rstrip('.').rstrip(',')\n",
    "    else:\n",
    "        return match2.group().rstrip().rstrip('.').rstrip(',')\n",
    "\n",
    "\n",
    "# def find_match_query_ver2(input_string):\n",
    "#     pattern = re.compile(r'MATCH.*?[.\\n]', re.DOTALL)\n",
    "#     matches = pattern.findall(input_string)\n",
    "\n",
    "#     if not matches:\n",
    "#         return None\n",
    "\n",
    "#     chosen_match = min(matches, key=len)\n",
    "#     return chosen_match.rstrip().rstrip('.').rstrip(',')\n",
    "\n",
    "\n",
    "def output_triples(output_query, record_names):\n",
    "    pattern = r'\\((\\w+):(\\w+)\\)-\\[:(\\w+)\\]->\\((\\w+):(\\w+)\\)\\s*RETURN (\\w+)'\n",
    "    match = re.search(pattern, output_query)\n",
    "    if match:\n",
    "        s, stype, p, o, otype, var = match.groups()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    if var == s:\n",
    "        return [(n, p, o) for n in record_names]\n",
    "    elif var == o:\n",
    "        return [(s, p, n) for n in record_names]\n",
    "    elif var == p:\n",
    "        return [(s, n, o) for n in record_names]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "DLjHrIUmFIEM",
   "metadata": {
    "id": "DLjHrIUmFIEM"
   },
   "outputs": [],
   "source": [
    "def retrieve_triples(question, model, node=node_str, rel=rel_str, api_token= \"apitoken\", max_new_token=100):\n",
    "\n",
    "\n",
    "#     prompt = f'''Cypher is a query language designed for querying graph databases. It was initially developed by Neo4j, and is now an open standard for various graph databases.\n",
    "\n",
    "# There are pre-defined node labels in our Neo4j database, namely {node}. There are also 15 pre-defined relationship types, namely {rel}.\n",
    "\n",
    "# Here's an example of a Cypher query for a natural language query \"What companies are the suppliers of Louis Vuitton?\":\n",
    "\n",
    "# MATCH (company:Company)-[:Supplies]->(Louis_Vuitton:Company) RETURN company\n",
    "\n",
    "# This query retrieves companies from the class \"Company\" that are connected with Intel by a \"Supplies\" relationship, and then returns those companies. Never forget to return values in a query.\n",
    "\n",
    "# Using the node labels and the relationship types in the database, for the query in natural language \"{question},\" the corresponding Cypher query should be '''\n",
    "\n",
    "\n",
    "    prompt = f'''Cypher is a query language designed for querying graph databases. It was initially developed by Neo4j, and is now an open standard for various graph databases.\n",
    "\n",
    "There are pre-defined node labels in our Neo4j database, namely {node}. There are also 15 pre-defined relationship types, namely {rel}.\n",
    "\n",
    "Here's an example of a Cypher query for a natural language query \"What companies are the suppliers of Louis Vuitton?\":\n",
    "MATCH (company:Company)-[:Supplies]->(Louis_Vuitton:Company) RETURN company\n",
    "This query retrieves companies from the class \"Company\" that are connected with Intel by a \"Supplies\" relationship, and then returns those companies.\n",
    "\n",
    "Here's another example for query \"Who does China trade with?\":\n",
    "MATCH (China:Country)-[:InteractsWith]->(other_country:Country) RETURN other_country\n",
    "This query retrieves countries from the class \"Country\" that are connected with China by a \"InteractsWith\" relationship, as the predicate \"trade with\" closely aligns with the pre-defined relationship type \"InteractsWith\".\n",
    "\n",
    "Never forget to return values in a query. Using the node labels and the relationship types in the database, for the query in natural language \"{question},\" the corresponding Cypher query should be '''\n",
    "\n",
    "    client = InferenceClient(model=model, token=api_token)\n",
    "    output_text = client.text_generation(prompt, max_new_tokens=max_new_token)\n",
    "    try:\n",
    "        output_query = find_match_query(output_text) + '.name'\n",
    "    except:\n",
    "        output_query = \"\"\n",
    "    try:\n",
    "        db_records, summary, keys = graphDB_Driver.execute_query(output_query, database_=\"neo4j\")\n",
    "        record_names = [r[keys[0]] for r in db_records]\n",
    "    except:\n",
    "        record_names = []\n",
    "    output = output_triples(output_query, record_names)\n",
    "    output_str = []\n",
    "    if output is not None:\n",
    "        for triplet in output:\n",
    "            if type(triplet[0])==str and type(triplet[1])==str and type(triplet[2])==str: #correct output type\n",
    "                output_str.append(triplet)\n",
    "\n",
    "\n",
    "\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "zekiQ_zqCWtp",
   "metadata": {
    "id": "zekiQ_zqCWtp"
   },
   "outputs": [],
   "source": [
    "def read_file_and_process_sentences(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, 'r',encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "\n",
    "            cleaned_line = line.strip()\n",
    "            if cleaned_line:  # Check if the line is not empty after stripping\n",
    "                sentences.append(cleaned_line)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a01bb6",
   "metadata": {
    "id": "04a01bb6"
   },
   "outputs": [],
   "source": [
    "def sentence_embeddings(sentences, model='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    # generate embedding vectors for sentences.\n",
    "    # sentences: list of input sentences,\n",
    "    # model: sentence-transformers/all-MiniLM-L6-v2 as default\n",
    "    model = SentenceTransformer(model)\n",
    "    embeddings = model.encode(sentences,batch_size=128)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "327d74a9",
   "metadata": {
    "id": "327d74a9"
   },
   "outputs": [],
   "source": [
    "def similarity_search(sentence_vectors, top_k=5, api_key=\"apikey\", environment=\"gcp-starter\", index=\"textsearch\"):\n",
    "    # return the top_k similar sentences for ench sentence_vector in inputs, inputs shape (n, )\n",
    "    # output list of shape (n, top_k)\n",
    "    pinecone.init(api_key=api_key,environment=environment)\n",
    "    index = pinecone.Index(\"textsearch\")\n",
    "    # print(index.describe_index_stats())\n",
    "\n",
    "    output_sentences=[]\n",
    "    for vector in sentence_vectors:\n",
    "        results = index.query(vector= vector.tolist(), top_k=top_k,include_metadata=True)\n",
    "        sentence=[]\n",
    "        for match_dict in results['matches']:\n",
    "            sentence.append(match_dict['metadata']['text'][1:-1]+\".\".strip())\n",
    "        output_sentences.append(sentence)\n",
    "    return output_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vo2XycrcUCk4",
   "metadata": {
    "id": "vo2XycrcUCk4"
   },
   "outputs": [],
   "source": [
    "# Some Back-up Prompts\n",
    "\n",
    "# prompt = f'''\n",
    "# RDF stands for Resource Description Framework, and it is a widely used format for representing information in the form of subject-predicate-object triplets in Knowledge Graph. Each RDF triplet consists of three components:\n",
    "\n",
    "# Subject: The subject is typically a resource or entity that you want to describe.\n",
    "# Predicate: The predicate, also known as the property or relationship, describes the relationship between the subject and the object.\n",
    "# Object: The object is the value or target of the statement.\n",
    "\n",
    "# Suppose you are an RDF triplet extraction model. I will give you 15 categories of predicates in the field of supply chain.\n",
    "\n",
    "# {category}\n",
    "\n",
    "# Give you some sentences, please extract all the RDF triplets based on the categories. Please form each triplet as (subject, predicate, object) + (predicate category) in the output. If no triplet is found, answer: None. Do not include other words in the answer.\n",
    "\n",
    "# Sentence: {sentence}\n",
    "\n",
    "# Triplets: '''\n",
    "\n",
    "\n",
    "# Prompt with no predicate types\n",
    "\n",
    "# prompt = f'''\n",
    "# In Knowledge Graphs, Resource Description Framework (RDF) serves as a widely adopted format for expressing information using subject-predicate-object triplets. Each RDF triplet consists of three primary components. The subject usually represents an entity. The predicate defines the connection between the subject and the object. The object is the value or target of the statement.\n",
    "\n",
    "# For instance, consider the sentence \"Apple contributes to the economy of China.\" In this case, the subject, predicate, and object are respectively \"Apple,\" \"contribute to,\" and \"the economy of China.\" We format the triplet output as (subject, predicate, object), which is (Apple, contribute to, the economy of China).\n",
    "\n",
    "# It's important to note that a single sentence can contain multiple triplets. For example, in the sentence, \"Walmart was trial-testing a service it developed with IBM to monitor produce,\" the output should consist of several triplets, including (Walmart, test, a service), (Walmart, develop with, IBM), and (Walmart, monitor, produce).\n",
    "\n",
    "# Given the above information, for the sentence \"{sentence},\" the correct output should be '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee199a01",
   "metadata": {
    "id": "ee199a01"
   },
   "outputs": [],
   "source": [
    "def triplet_extraction(sentences, start_idx, end_idx, category, model, api_token=\"apitoken\", max_new_token=100):\n",
    "\n",
    "    # extract triples of given setences, using giving model:\"meta-llama/Llama-2-70b-chat-hf\",\"meta-llama/Llama-2-70b-chat-hf\", \"bigscience/bloom-560m\",\"bigscience/bloom-3b\"\n",
    "    # input shape (n,1), output an excel with column：sentence_index\tsubject\tpredicate\tobject\tpredicate_type\n",
    "\n",
    "    client = InferenceClient(model=model, token=api_token)\n",
    "\n",
    "    pattern = r'\\(([^,]+), ([^,]+), ([^)]+)\\) \\+ \\(([^)]+)\\)'\n",
    "    # pattern = r'\\(([^,]+), ([^,]+), ([^)]+)\\)'\n",
    "\n",
    "    df = pd.DataFrame(columns = ['sentence_index', 'subject', 'predicate', 'object', 'predicate_type'])\n",
    "    # df = pd.DataFrame(columns = ['sentence_index', 'subject', 'predicate', 'object'])\n",
    "\n",
    "    sentence_idx = start_idx\n",
    "\n",
    "    for sentence in sentences[start_idx:end_idx]:\n",
    "\n",
    "        # Use a few-shot prompt\n",
    "        prompt = f'''\n",
    "In Knowledge Graphs, Resource Description Framework (RDF) serves as a widely adopted format for expressing information using subject-predicate-object triplets. Each RDF triplet consists of three primary components. The subject usually represents an entity. The predicate defines the connection between the subject and the object. The object is the value or target of the statement.\n",
    "\n",
    "In the field of supply chain, there are 15 pre-defined categories of predicates, which are as follows.\n",
    "\n",
    "{category}\n",
    "\n",
    "For instance, consider the sentence \"Apple contributes to the economy of China.\" In this case, the subject, predicate, and object are respectively \"Apple,\" \"contribute to,\" and \"the economy of China.\" The predicate \"contribute to\" belongs to the category of \"Impacts.\" Consequently, we format the triplet output as (subject, predicate, object) + (predicate category), which is (Apple, contribute to, the economy of China) + (Impacts) for this sentence.\n",
    "\n",
    "It's important to note that a single sentence can contain multiple triplets. For example, in the sentence, \"Walmart was trial-testing a service it developed with IBM to monitor produce,\" the output should consist of several triplets, including (Walmart, trial-test, a service) + (Assesses), (Walmart, develop with, IBM) + (Develops), (Walmart, monitor, production) + (Assesses).\n",
    "\n",
    "Given the above information, for the sentence \"{sentence},\" the correct output should be '''\n",
    "\n",
    "        output_text = client.text_generation(prompt, max_new_tokens=max_new_token)\n",
    "        # print(sentence)\n",
    "        # print(output_text)\n",
    "\n",
    "        matches = re.findall(pattern, output_text)\n",
    "        for m in matches:\n",
    "            phrase1, phrase2, phrase3, phrase4 = m\n",
    "            new_row = {'sentence_index': sentence_idx, 'subject': phrase1, 'predicate': phrase2, 'object': phrase3, 'predicate_type': phrase4}\n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "\n",
    "        sentence_idx += 1\n",
    "\n",
    "    return df\n",
    "    # df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b6b015",
   "metadata": {
    "id": "16b6b015"
   },
   "outputs": [],
   "source": [
    "def chatbot_pipeline(questions, start_idx, end_idx, model, category, max_new_token=300, api_token=\"apitoken\"):\n",
    "\n",
    "    # generate answers for questions\n",
    "    # store triplets for questions and triplets,output answers in answers.txt\n",
    "    model_name=model.split(\"/\")[-1]\n",
    "    client = InferenceClient(model=model, token=api_token)\n",
    "    # triplet retrival\n",
    "    #If there are relevant triplets,keep the triplets,else keep the question for sentence similarity search\n",
    "\n",
    "    print(\"Triplet Retrieval\")\n",
    "\n",
    "    sentences_list=[]\n",
    "\n",
    "    for i in tqdm(range(start_idx, end_idx)):\n",
    "        triplet_list = retrieve_triples(question=questions[i], model=model)\n",
    "        \n",
    "        print(i,triplet_list)\n",
    "\n",
    "        if triplet_list is not None and len(triplet_list)!=0:\n",
    "            triplet_list_sentence = [\" \".join(triplet_list[i])+\".\" for i in range(0,len(triplet_list))] #list of sentences\n",
    "            sentences_list.append(triplet_list_sentence)\n",
    "        else:\n",
    "            sentences_list.append([questions[i]]) #None relevant triplets,keep the questions\n",
    "   # print(\"sentence_list:\",sentences_list)\n",
    "\n",
    "    print(\"Sentence Embedding and Similarity search\")\n",
    "    sentence_vectors=[]\n",
    "    background_sentences=[]\n",
    "    for i in tqdm(range(0,len(sentences_list))):\n",
    "        #print(sentences_list[i])\n",
    "        sentence_vector = sentence_embeddings(sentences_list[i])\n",
    "        #print(len(sentence_vector))\n",
    "        sentence_vectors.append(sentence_vector)\n",
    "        if len(sentence_vector)==1:\n",
    "\n",
    "            background_sentence = list(set(similarity_search(sentence_vector,top_k=5)[0])) #more sentences for questions\n",
    "\n",
    "        else:\n",
    "            background_sentence = list(set([sentence[0] for sentence in similarity_search(sentence_vector,top_k=1)]))\n",
    "            if len(background_sentence)>=20:\n",
    "                #chunk background sentence\n",
    "                background_sentence=background_sentence[0:20] \n",
    "                \n",
    "        background_sentences.append(background_sentence)\n",
    "        #print(\"back:\",background_sentences)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"back:\",background_sentences)\n",
    "    # join background sentences\n",
    "    background_sentences_join=[\"\" for i in range(0,len(background_sentences))]\n",
    "    answers=[]\n",
    "    for i in range(0, len(background_sentences)):\n",
    "          background_sentences_join[i]= (\"\\n\").join(background_sentences[i]) #join the relevant background information\n",
    "    #print(background_sentences)\n",
    "    \n",
    "    id=start_idx\n",
    "    with open(model_name+\"_\"+\"background_sentence.txt\", \"a\",encoding='utf-8') as file:\n",
    "        for sentence in background_sentences_join:\n",
    "            file.write(str(id)+\"<SEP>\"+'\\n')\n",
    "            file.write(sentence + '\\n')\n",
    "            id=id+1\n",
    "\n",
    "    print(\"Extracting triple from questions\")\n",
    "    df_questions = pd.DataFrame(columns = ['question_index','sentence_index', 'subject', 'predicate', 'object', 'predicate_type'])#might overflow\n",
    "    # df_questions = pd.DataFrame(columns = ['question_index','sentence_index', 'subject', 'predicate', 'object'])#might overflow\n",
    "    for i in tqdm(range(start_idx, end_idx)):\n",
    "        df = triplet_extraction(background_sentences[i-start_idx],start_idx=0, end_idx=len(background_sentences[i-start_idx]), category=category,model=model)\n",
    "        df['question_index'] = [i for j in range(0,len(df))]\n",
    "        df = df[['question_index', 'sentence_index', 'subject', 'predicate', 'object', 'predicate_type']]\n",
    "        # df = df[['question_index', 'sentence_index', 'subject', 'predicate', 'object']]\n",
    "        df_questions = pd.concat([df_questions,df], ignore_index=True)\n",
    "    df_questions = df_questions.drop('sentence_index', axis=1)\n",
    "    df_questions.to_csv(model_name+\"_\"+f'''questions_triples_{start_idx}_{end_idx}.csv''', index=False)\n",
    "\n",
    "    print(\"Generating answers\")\n",
    "    for i in tqdm(range(start_idx,end_idx)):\n",
    "        prompt = f'''\n",
    "        I will give you a question and related information. Please answer the question based on the given related information. Do not include other information.\n",
    "        Question: {questions[i]}\n",
    "        Related information:{background_sentences_join[i-start_idx]}\n",
    "\n",
    "        Answer:'''\n",
    "        output_text = client.text_generation(prompt,max_new_tokens=max_new_token)\n",
    "        # print(prompt)\n",
    "        # print(output_text)\n",
    "        answers.append(output_text.strip())\n",
    "    id=start_idx\n",
    "    with open(model_name+\"_\"+\"answers.txt\", \"a\",encoding='utf-8') as file:\n",
    "        for answer in answers:\n",
    "            file.write(str(id)+\"<SEP>\" '\\n')\n",
    "            file.write(answer + '\\n')\n",
    "            id=id+1\n",
    "\n",
    "    print(\"Extracting triple from answers\")\n",
    "    answer_sentences=[answers[i].split(\".\") for i in range(0,len(answers))]\n",
    "    df_answers = pd.DataFrame(columns = ['answer_index','sentence_index', 'subject', 'predicate', 'object', 'predicate_type']) # might overflow\n",
    "    # df_answers = pd.DataFrame(columns = ['answer_index','sentence_index', 'subject', 'predicate', 'object']) # might overflow\n",
    "    for i in tqdm(range(start_idx,end_idx)):\n",
    "\n",
    "        df=triplet_extraction(answer_sentences[i-start_idx],start_idx=0,end_idx=len(answer_sentences[i-start_idx]), category=category,model=model)\n",
    "        df['answer_index'] = [i for j in range(0,len(df))]\n",
    "        df = df[['answer_index','sentence_index', 'subject', 'predicate', 'object', 'predicate_type']]\n",
    "        # df = df[['answer_index','sentence_index', 'subject', 'predicate', 'object']]\n",
    "        df_answers=pd.concat([df_answers,df], ignore_index=True)\n",
    "    df_answers=df_answers.drop('sentence_index', axis=1)\n",
    "\n",
    "    df_answers.to_csv(model_name+\"_\"+f'''answers_triples_{start_idx}_{end_idx}.csv''', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9uSsgpzOVKU5",
   "metadata": {
    "id": "9uSsgpzOVKU5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6DNbY54BVKXN",
   "metadata": {
    "id": "6DNbY54BVKXN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3020e44",
   "metadata": {
    "id": "c3020e44"
   },
   "outputs": [],
   "source": [
    "category = '''Category 1: ActTowards\n",
    "Produce, create, present, implement, construct, operate, perform, conduct, run, tend, carry, start, build\n",
    "\n",
    "Category 2: CommunicatesWith\n",
    "describe, express, indicate, note, suggest, propose, discuss, explain, report, represent, illustrate, refer, mention, reveal, specify\n",
    "\n",
    "Category 3: InteractsWith\n",
    "connect, relate, associate, link, integrate, combine, incorporate, meet, interact, engage, cooperate, communicate, collaborate\n",
    "\n",
    "Category 4: Supplies\n",
    "give, share, offer, provide, supply, deliver\n",
    "\n",
    "Category 5: Demands\n",
    "need, require, demand, depend\n",
    "\n",
    "Category 6: Acquires\n",
    "order, obtain, purchase, buy, acquire, receive, trade, sell\n",
    "\n",
    "Category 7: Transforms\n",
    "change, transform, adapt, shift\n",
    "\n",
    "Category 8: Decides\n",
    "choose, decide, select, pick, opt\n",
    "\n",
    "Category 9: Assesses\n",
    "measure, assess, evaluate, calculate, test, examine, investigate\n",
    "\n",
    "Category 10: Solves\n",
    "solve, fix, resolve, address\n",
    "\n",
    "Category 11: Develops\n",
    "increase, develop, improve, grow, progress, advance, evolve, proceed\n",
    "\n",
    "Category 12: Impacts\n",
    "affect, influence, Impact, contribute, facilitate\n",
    "\n",
    "Category 13: Manages\n",
    "manage, control, coordinate, regulate, arrange, organize, sort\n",
    "\n",
    "Category 14: Moves\n",
    "move, go, travel, journey, walk\n",
    "\n",
    "Category 15: Happens\n",
    "occur, happen, continue, remain'''"
   ]
  }
 
